{"id":3529,"date":"2020-07-14T10:43:46","date_gmt":"2020-07-14T10:43:46","guid":{"rendered":"http:\/\/davidegerosa.com\/?page_id=3529"},"modified":"2021-02-21T08:13:28","modified_gmt":"2021-02-21T08:13:28","slug":"pdetclassifier","status":"publish","type":"page","link":"https:\/\/davidegerosa.com\/pdetclassifier\/","title":{"rendered":"pdetclassifier: Gravitational-wave selection effects using neural-network"},"content":{"rendered":"\n<p>Check me out on <a href=\"https:\/\/github.com\/dgerosa\/pdetclassifier\">github.com\/dgerosa\/pdetclassifier<\/a>.<\/p>\n\n\n\n<blockquote>\n  <p>We present a novel machine-learning approach to estimate  selection biases in gravitational-wave observations. Using techniques similar to those commonly employed in image classification and pattern recognition, we train a series of neural-network classifiers to predict the LIGO\/Virgo detectability of gravitational-wave signals from compact-binary mergers. We include the effect of spin precession, higher-order modes, and multiple detectors and show that their omission, as it is common in large population studies, tends to overestimate the inferred merger rate. Although here we train our classifiers using a simple signal-to-noise ratio threshold, our approach is ready to be used in conjunction with full pipeline injections, thus paving the way toward including empirical distributions of  astrophysical and noise triggers into gravitational-wave population analyses.<\/p>\n<\/blockquote>\n\n<p>This repository contains models supporting <a href=\"https:\/\/arxiv.org\/abs\/2007.06585\">arXiv:2007.06585<\/a>. We are very happy if you find this useful for your research; please cite our paper. For a DOI pointing to this repository: <a href=\"https:\/\/zenodo.org\/badge\/latestdoi\/278096080\"><img decoding=\"async\" src=\"https:\/\/zenodo.org\/badge\/278096080.svg\" alt=\"DOI\" \/><\/a><\/p>\n\n<p>This code is developed and maintained by <a href=\"https:\/\/davidegerosa.com\/\">Davide Gerosa<\/a>. To report bugs, please open an issue on GitHub. If you want to contact me, it's <code>d.gerosa@bham.ac.uk<\/code>.<\/p>\n\n<h2>Data products<\/h2>\n\n<p>We provide three kinds of data products:<\/p>\n\n<ul>\n<li>Our code: <code>pdetclassifier.py<\/code>. See below for a short description.<\/li>\n<li>Pre-trained TensorFlow neural networks: <code>trained_*.h5<\/code>.<\/li>\n<li>Training\/validation samples: <code>sample_*.h5<\/code>.These can be downloaded from the <a href=\"https:\/\/github.com\/dgerosa\/pdetclassifier\/releases\">github release page<\/a>.<\/li>\n<\/ul>\n\n<p>Models were trained on samples of N=2e7 binaries. This sample is divided in two chunks of 1e7 sources each used for training and validation.<\/p>\n\n<p>The following models are described in <a href=\"https:\/\/arxiv.org\/abs\/2007.06585\">arXiv:2007.06585<\/a>. \n- <code>trained_2e7_design_nonspinning_quadrupole_1detector.h5<\/code>\n- <code>trained_2e7_design_precessing_higherordermodes_1detector.h5<\/code>\n- <code>trained_2e7_design_precessing_higherordermodes_3detectors.h5<\/code>\nThey are computed assuming LIGO\/Virgo noise curves <code>aLIGODesignSensitivityP1200087<\/code> and <code>AdVDesignSensitivityP1200087<\/code> from <code>lal<\/code>.<\/p>\n\n<p>The following additional models use noise curves for LIGO\/Virgo O1+O2, O3, and forecasted O4. The training distributions and the network setup is the same as described in the paper. \n- <code>trained_2e7_O1O2_precessing_higherordermodes_3detectors.h5<\/code>\n- <code>trained_2e7_O3_precessing_higherordermodes_3detectors.h5<\/code>\n- <code>trained_2e7_O4_precessing_higherordermodes_3detectors.h5<\/code>\nFor O1+O2 we use the <code>aLIGOEarlyHighSensitivityP1200087<\/code> and <code>AdVEarlyHighSensitivityP1200087<\/code> noise curves from <code>lal<\/code>. For O3 and O4 we use the txt files from <a href=\"https:\/\/dcc.ligo.org\/LIGO-T2000012\/public\">LIGO-T2000012<\/a>.<\/p>\n\n<h2>Code and examples<\/h2>\n\n<p>First, install the following python packages: <code>tensorflow<\/code>, <code>astropy<\/code>, <code>lalsuite<\/code>, <code>pycbc<\/code>, <code>tqdm<\/code>, and <code>deepdish<\/code>.<\/p>\n\n<p><em>Note<\/em>: if used as it is, the <code>pdetclassifier.py<\/code> script assumes precessing systems, higher-order modes, and a three-detector network. If you want to do something different, you'll need to hack it a little bit.<\/p>\n\n<h3>Example 1: use a precomputed model<\/h3>\n\n<p>Here is a code snippet to use a precomputed model:<\/p>\n\n<pre><code># Load sample\nbinaries= readsample('sample_2e7_design_precessing_higherordermodes_3detectors.h5')\n# Split test\/training\ntrain_binaries,test_binaries=splittwo(binaries)\n# Load trained network\nmodel = loadnetwork('trained_2e7_design_precessing_higherordermodes_3detectors.h5')\n# Evaluate performances on training sample\ntestnetwork(model,train_binaries)\n# Evaluate performances on test sample\ntestnetwork(model,test_binaries)\n# Predict on new sample\nnewbinaries = generate_binaries(100)\npredictions = predictnetwork(model, newbinaries)\nprint(predictions)\n# Regenerate the extrinsic angles and marginalize over them\npdets = pdet(model,newbinaries, Nmc=1000)\nprint(pdets)\n<\/code><\/pre>\n\n<p>The <code>binaries<\/code> object is a python dictionary with keys \n- <code>mtot<\/code>: detector-frame total mass\n- <code>q<\/code>: mass ratio\n- <code>z<\/code>: redshift\n- <code>chi1x<\/code>,<code>chi1y<\/code>,<code>chi1z<\/code>: dimensionless spin components of the primary\n- <code>chi2x<\/code>,<code>chi2y<\/code>,<code>chi2z<\/code>: dimensionless spin components of the secondary\n- <code>iota<\/code>: inclidation\n- <code>ra<\/code>,<code>dec<\/code>: sky location\n- <code>psi<\/code>: polarization.\n- <code>snr<\/code>: the SNR\n- <code>det<\/code>: detectability, equal to 1 if detectable or 0 if not detectable.\nThe frame of the spins is defined such that z is along L at 20 Hz (as in <code>lal<\/code>).<\/p>\n\n<p>The <code>predictions<\/code> one gets at the end is a list of 0s and 1s, encoding the predicted detectability. One can then marginalize over the extrinsic angles to compute the detection probability pdet (by default the <code>pdet<\/code> function assumes isotropic inclination, sky-location and polarization, <code>Nmc<\/code> is the number of Monte Carlo samples).<\/p>\n\n<h3>Example 2: train your own neural network<\/h3>\n\n<p>Here is an example where we generate a small sample of 1000 binaries, train a neural network, and evaluate the performances.<\/p>\n\n<pre><code># Generate and store a sample\nstore_binaries('sample.h5',1e3,approximant='IMRPhenomXPHM',noisecurve='design',SNRthreshold=12)\n# Load sample\nbinaries= readsample('sample.h5')\n# Split test\/training\ntrain_binaries,test_binaries=splittwo(binaries)\n# Train a neural network\ntrainnetwork(train_binaries,test_binaries,filename='trained.h5')\n# Load trained network\nmodel = loadnetwork('trained.h5')\n# Evaluate performances on training sample\ntestnetwork(model,train_binaries)\n# Evaluate performances on test sample\ntestnetwork(model,test_binaries)\n# Predict on new sample\nnewbinaries = generate_binaries(100)\npredictions = predictnetwork(model, newbinaries)\nprint(predictions)\n# Regenerate the extrinsic angles and marginalize over them\npdets = pdet(model,newbinaries, Nmc=1000)\nprint(pdets)\n<\/code><\/pre>\n\n<h3>Example 3: evaluate selection effects for your population<\/h3>\n\n<p>Here is an example where we just compute the detectability of an injected population using one of our neural networks<\/p>\n\n<pre><code># Initialize\nbinaries={}\nN = int(1e3)\nbinaries = generate_binaries(N)\n# Populate with your distribution\nbinaries['mtot'] = np.random.normal(30, 3, N )\nbinaries['q'] = np.random.uniform(0.1,1,N)\nbinaries['z'] = np.random.normal(0.2, 0.01, N )\nbinaries['chi1x'] = np.random.uniform(0, 0.1, N )\nbinaries['chi1y'] = np.random.uniform(0, 0.1, N )\nbinaries['chi1z'] = np.random.uniform(0, 0.1, N )\nbinaries['chi2x'] = np.random.uniform(0, 0.1, N )\nbinaries['chi2y'] = np.random.uniform(0, 0.1, N )\nbinaries['chi2z'] = np.random.uniform(0, 0.1, N )\n# Load trained network\nmodel = loadnetwork('trained_2e7_design_precessing_higherordermodes_3detectors.h5')\n# Compute detectability averaged over extrinsic parameters\npdets = pdet(model,binaries, Nmc=1000)\nprint(pdets)\n# Integrate over entire population\npredictions = predictnetwork(model, binaries)\nintegral = np.sum(predictions)\/N\nprint(integral)\n<\/code><\/pre>\n\n\n\n\n<p><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Check me out on github.com\/dgerosa\/pdetclassifier.<\/p>\n","protected":false},"author":2,"featured_media":3534,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_coblocks_attr":"","_coblocks_dimensions":"","_coblocks_responsive_height":"","_coblocks_accordion_ie_support":"","_monsterinsights_skip_tracking":false,"_monsterinsights_sitenote_active":false,"_monsterinsights_sitenote_note":"","_monsterinsights_sitenote_category":0,"footnotes":""},"jetpack_sharing_enabled":true,"_links":{"self":[{"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/pages\/3529"}],"collection":[{"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/comments?post=3529"}],"version-history":[{"count":4,"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/pages\/3529\/revisions"}],"predecessor-version":[{"id":3802,"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/pages\/3529\/revisions\/3802"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/media\/3534"}],"wp:attachment":[{"href":"https:\/\/davidegerosa.com\/wp-json\/wp\/v2\/media?parent=3529"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}